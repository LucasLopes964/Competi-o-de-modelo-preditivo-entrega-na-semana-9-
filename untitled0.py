# -*- coding: utf-8 -*-
"""Untitled0.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1QBpSei-tbgRvD8VWQL5St3Fw95Rruq5f
"""

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.preprocessing import StandardScaler
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import classification_report, roc_auc_score
import warnings
warnings.filterwarnings('ignore')

train = pd.read_csv('train.csv')
test = pd.read_csv('test.csv')
sample_sub = pd.read_csv('sample_submission.csv')
print(f"Train shape: {train.shape}")
print(f"Test shape: {test.shape}")
print(f"Target distribution:\n{train['labels'].value_counts(normalize=True)}")

print("\nMissing values por coluna:")
print(train.isnull().sum().sort_values(ascending=False).head(10))

age_cols = ['age_first_funding_year', 'age_last_funding_year',
           'age_first_milestone_year', 'age_last_milestone_year']

for col in age_cols:
    train[col].fillna(train[col].median(), inplace=True)
    test[col].fillna(train[col].median(), inplace=True)

train['funding_total_usd'].fillna(train['funding_total_usd'].median(), inplace=True)
test['funding_total_usd'].fillna(train['funding_total_usd'].median(), inplace=True)

train_encoded = pd.get_dummies(train, columns=['category_code'], prefix='cat')
test_encoded = pd.get_dummies(test, columns=['category_code'], prefix='cat')

train_encoded, test_encoded = train_encoded.align(test_encoded, join='left', axis=1, fill_value=0)

numeric_cols = ['relationships', 'funding_rounds', 'funding_total_usd',
               'milestones', 'avg_participants'] + age_cols
scaler = StandardScaler()
train_encoded[numeric_cols] = scaler.fit_transform(train_encoded[numeric_cols])
test_encoded[numeric_cols] = scaler.transform(test_encoded[numeric_cols])

X = train_encoded.drop(['labels'], axis=1)
y = train_encoded['labels']
if 'labels' in test_encoded.columns:
    test_encoded = test_encoded.drop(['labels'], axis=1)
X_test = test_encoded
X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2,
                                                  random_state=42, stratify=y)
print(f"X shape: {X.shape}")
print(f"X_train shape: {X_train.shape}")
print(f"X_val shape: {X_val.shape}")
print(f"X_test shape: {X_test.shape}")

from sklearn.model_selection import GridSearchCV
param_grid_rf = {
    'n_estimators': [200, 500],
    'max_depth': [10, 20, None],
    'min_samples_split': [2, 5, 10],
    'min_samples_leaf': [1, 2, 4],
    'max_features': ['sqrt', 'log2']
}
print(":lupa: Rodando GridSearchCV para Random Forest...")
grid_search = GridSearchCV(
    RandomForestClassifier(random_state=42, class_weight='balanced'),
    param_grid_rf,
    cv=5,
    n_jobs=-1,
    scoring='accuracy',
    verbose=1
)
grid_search.fit(X, y)
print(":marca_de_verificação_branca: Melhores parâmetros encontrados:", grid_search.best_params_)
print(":marca_de_verificação_branca: Acurácia CV:", grid_search.best_score_)
best_rf_model = grid_search.best_estimator_

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.ensemble import RandomForestClassifier, StackingClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import classification_report, roc_auc_score
from lightgbm import LGBMClassifier
import xgboost as xgb
from imblearn.over_sampling import SMOTE
import warnings
warnings.filterwarnings('ignore')
print(":foguete: PIPELINE ULTRA OTIMIZADO PARA BATER 80%+ DE ACURÁCIA")
print("=" * 60)
train = pd.read_csv('train.csv')
test = pd.read_csv('test.csv')
sample_sub = pd.read_csv('sample_submission.csv')
print(f":gráfico_de_barras: Train shape: {train.shape}")
print(f":gráfico_de_barras: Test shape: {test.shape}")
print(f":gráfico_de_barras: Target distribution:\n{train['labels'].value_counts(normalize=True)}")
print("\n:chave_inglesa: Iniciando pré-processamento avançado...")
age_cols = ['age_first_funding_year', 'age_last_funding_year',
           'age_first_milestone_year', 'age_last_milestone_year']
for col in age_cols:
    train[col] = train[col].fillna(train[col].median())
    test[col] = test[col].fillna(train[col].median())
train['funding_total_usd'] = train['funding_total_usd'].fillna(train['funding_total_usd'].median())
test['funding_total_usd'] = test['funding_total_usd'].fillna(train['funding_total_usd'].median())
print(":foguete: Criando features avançadas...")
train['funding_per_round'] = train['funding_total_usd'] / (train['funding_rounds'] + 1e-6)
test['funding_per_round'] = test['funding_total_usd'] / (test['funding_rounds'] + 1e-6)
train['milestones_per_year'] = train['milestones'] / (train['age_last_milestone_year'] + 1e-6)
test['milestones_per_year'] = test['milestones'] / (test['age_last_milestone_year'] + 1e-6)
train['has_multiple_rounds'] = (train['funding_rounds'] > 1).astype(int)
test['has_multiple_rounds'] = (test['funding_rounds'] > 1).astype(int)
train['funding_age_span'] = train['age_last_funding_year'] - train['age_first_funding_year']
test['funding_age_span'] = test['age_last_funding_year'] - test['age_first_funding_year']
train['relationships_per_milestone'] = train['relationships'] / (train['milestones'] + 1e-6)
test['relationships_per_milestone'] = test['relationships'] / (test['milestones'] + 1e-6)
train['funding_efficiency'] = train['funding_total_usd'] / (train['relationships'] + 1e-6)
test['funding_efficiency'] = test['funding_total_usd'] / (test['relationships'] + 1e-6)
train['milestone_density'] = train['milestones'] / (train['funding_rounds'] + 1e-6)
test['milestone_density'] = test['funding_rounds'] / (test['funding_rounds'] + 1e-6)
train['avg_funding_per_participant'] = train['funding_total_usd'] / (train['avg_participants'] + 1e-6)
test['avg_funding_per_participant'] = test['funding_total_usd'] / (test['avg_participants'] + 1e-6)
train['funding_milestone_ratio'] = train['funding_total_usd'] / (train['milestones'] + 1e-6)
test['funding_milestone_ratio'] = test['funding_total_usd'] / (test['milestones'] + 1e-6)
train.replace([np.inf, -np.inf], np.nan, inplace=True)
test.replace([np.inf, -np.inf], np.nan, inplace=True)
train.fillna(0, inplace=True)
test.fillna(0, inplace=True)
print(f":marca_de_verificação_branca: Features criadas! Train shape: {train.shape}")
print(":sentido_anti_horário: Aplicando encoding e scaling...")
train_encoded = pd.get_dummies(train, columns=['category_code'], prefix='cat')
test_encoded = pd.get_dummies(test, columns=['category_code'], prefix='cat')
train_encoded, test_encoded = train_encoded.align(test_encoded, join='left', axis=1, fill_value=0)
new_features = ['funding_per_round', 'milestones_per_year', 'has_multiple_rounds',
                'funding_age_span', 'relationships_per_milestone', 'funding_efficiency',
                'milestone_density', 'avg_funding_per_participant', 'funding_milestone_ratio']
numeric_cols = ['relationships', 'funding_rounds', 'funding_total_usd',
               'milestones', 'avg_participants'] + age_cols + new_features
scaler = StandardScaler()
train_encoded[numeric_cols] = scaler.fit_transform(train_encoded[numeric_cols])
test_encoded[numeric_cols] = scaler.transform(test_encoded[numeric_cols])
X = train_encoded.drop(['labels'], axis=1)
y = train_encoded['labels']
if 'labels' in test_encoded.columns:
    test_encoded = test_encoded.drop(['labels'], axis=1)
X_test = test_encoded
print(f":marca_de_verificação_branca: Dataset final: X shape = {X.shape}, X_test shape = {X_test.shape}")
X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2,
                                                  random_state=42, stratify=y)
print(f":gráfico_de_barras: Split: Train={X_train.shape[0]}, Val={X_val.shape[0]}")
print("\n:balança: Aplicando SMOTE para balanceamento...")
sm = SMOTE(random_state=42)
X_train_balanced, y_train_balanced = sm.fit_resample(X_train, y_train)
print(f":gráfico_de_barras: Antes SMOTE: {y_train.value_counts().to_dict()}")
print(f":gráfico_de_barras: Depois SMOTE: {y_train_balanced.value_counts().to_dict()}")
print("\n:robô_cabeça: Treinando modelos individuais otimizados...")
rf_model = RandomForestClassifier(
    n_estimators=800,
    max_depth=25,
    max_features='sqrt',
    min_samples_split=2,
    min_samples_leaf=1,
    bootstrap=True,
    random_state=42,
    n_jobs=-1,
    class_weight='balanced'
)

lgbm_model = LGBMClassifier(
    n_estimators=1000,
    max_depth=25,
    learning_rate=0.03,
    subsample=0.8,
    colsample_bytree=0.8,
    reg_alpha=0.1,
    reg_lambda=0.1,
    class_weight='balanced',
    random_state=42,
    verbose=-1
)

xgb_model = xgb.XGBClassifier(
    n_estimators=800,
    max_depth=8,
    learning_rate=0.03,
    subsample=0.8,
    colsample_bytree=0.8,
    reg_alpha=0.1,
    reg_lambda=0.1,
    random_state=42,
    scale_pos_weight=1,
    eval_metric='logloss',
    use_label_encoder=False
)
print(":fogo: Treinando Random Forest...")
rf_model.fit(X_train_balanced, y_train_balanced)
print(":fogo: Treinando LightGBM...")
lgbm_model.fit(X_train_balanced, y_train_balanced)
print(":fogo: Treinando XGBoost...")
xgb_model.fit(X_train_balanced, y_train_balanced)
print(":marca_de_verificação_branca: Modelos individuais treinados!")

print("\n:foguete: Criando Stacking Classifier...")

meta_model = LogisticRegression(
    max_iter=1000,
    random_state=42,
    class_weight='balanced'
)

stacking_model = StackingClassifier(
    estimators=[
        ('rf', rf_model),
        ('lgbm', lgbm_model),
        ('xgb', xgb_model)
    ],
    final_estimator=meta_model,
    passthrough=True,
    cv=5,
    n_jobs=-1
)
print(":fogo: Treinando Stacking Classifier...")
stacking_model.fit(X_train_balanced, y_train_balanced)
print(":marca_de_verificação_branca: Stacking Classifier treinado!")

print("\n:dardo_no_alvo: THRESHOLD TUNING ULTRA FINO...")
print("=" * 50)

y_prob_val = stacking_model.predict_proba(X_val)[:, 1]

best_acc = 0
best_thr = 0.5
print("Testando thresholds de 0.30 a 0.80 com passo 0.001...")
for thr in np.arange(0.30, 0.81, 0.001):
    preds_thr = (y_prob_val > thr).astype(int)
    acc = (preds_thr == y_val).mean()
    if acc > best_acc:
        best_acc = acc
        best_thr = thr
print(f"\n:dardo_no_alvo: MELHOR THRESHOLD: {best_thr:.3f}")
print(f":dardo_no_alvo: MELHOR ACCURACY: {best_acc:.4f} ({best_acc*100:.1f}%)")

if best_acc >= 0.80:
    print(":confete_e_serpentina::confete_e_serpentina::confete_e_serpentina: SUCESSO! BATEMOS OS 80% DE ACURÁCIA! :confete_e_serpentina::confete_e_serpentina::confete_e_serpentina:")
else:
    print(f":atenção:  Quase lá! Faltam {(0.80 - best_acc)*100:.1f} pontos percentuais para 80%")

y_pred_final = (y_prob_val > best_thr).astype(int)
auc_final = roc_auc_score(y_val, y_prob_val)
print(f"\n:gráfico_de_barras: RELATÓRIO FINAL (threshold = {best_thr:.3f}):")
print(f"AUC: {auc_final:.4f}")
print(classification_report(y_val, y_pred_final))

print("\n:foguete: TREINANDO MODELO FINAL E GERANDO SUBMISSÃO...")

final_stacking = StackingClassifier(
    estimators=[
        ('rf', rf_model),
        ('lgbm', lgbm_model),
        ('xgb', xgb_model)
    ],
    final_estimator=meta_model,
    passthrough=True,
    cv=5,
    n_jobs=-1
)

X_full_balanced, y_full_balanced = sm.fit_resample(X, y)

final_stacking.fit(X_full_balanced, y_full_balanced)

y_prob_test = final_stacking.predict_proba(X_test)[:, 1]
test_predictions = (y_prob_test > best_thr).astype(int)

submission = pd.DataFrame({
    'id': sample_sub['id'],
    'labels': test_predictions
})
submission.to_csv('my_submission_ultra_optimized.csv', index=False)
print(":marca_de_verificação_branca: Submissão salva como 'my_submission_ultra_optimized.csv'")
print(f":marca_de_verificação_branca: Modelo usado: Stacking Classifier (RF + LGBM + XGB)")
print(f":marca_de_verificação_branca: Balanceamento: SMOTE")
print(f":marca_de_verificação_branca: Threshold otimizado: {best_thr:.3f}")
print(f":marca_de_verificação_branca: Acurácia alcançada: {best_acc:.1%}")
print(f":marca_de_verificação_branca: AUC: {auc_final:.4f}")

print(f"\n:gráfico_de_barras: Distribuição das predições no test set:")
print(f"Classe 0 (insucesso): {(test_predictions == 0).sum()} startups")
print(f"Classe 1 (sucesso): {(test_predictions == 1).sum()} startups")
print(f"Proporção de sucesso: {test_predictions.mean():.1%}")
print("\n" + "="*60)
print(":dardo_no_alvo: RESUMO FINAL:")
print("="*60)
print(f"• Features criadas: {len(new_features)} novas variáveis")
print(f"• Balanceamento: SMOTE aplicado")
print(f"• Modelos: Random Forest + LightGBM + XGBoost")
print(f"• Meta-modelo: Stacking Classifier")
print(f"• Threshold: {best_thr:.3f} (otimizado com passo 0.001)")
print(f"• Acurácia final: {best_acc:.1%}")
print(f"• AUC: {auc_final:.4f}")
print("="*60)